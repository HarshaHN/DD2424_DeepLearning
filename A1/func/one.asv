% DD2424 Deep Learning in Data Science from Prof. Josephine Sullivan
% 01 Assignment dated March 19 2019 
% Author: Harsha HN harshahn@kth.se
% Mini-batch gradient descent algorithm
% Exercise 1

function one
    close all; clear all; clc;

    k = 10; %class 
    d = 32*32*3; %image size
    N = 10000; %Num of image

    %Load the datasets
    [X, Y, ~] = LoadBatch('./Datasets/cifar-10-batches-mat/data_batch_1.mat');
    [Xv, Yv, ~] = LoadBatch('./Datasets/cifar-10-batches-mat/data_batch_2.mat');
    [Xt, ~, yt] = LoadBatch('./Datasets/cifar-10-batches-mat/test_batch.mat');
    % X: 3072x10,000, Y: 10x10,000, y: 1x10,000

    % Initialization of parameters & hyperparameters
    [W, b] = InitParam(k, d); lambda = 0; % W: 10x3072, b: 10x1
    GDparams.n_batch = 100; GDparams.eta = 0.1; GDparams.n_epochs = 40;
    J_train = zeros(1, GDparams.n_epochs); J_val = zeros(1, GDparams.n_epochs);

    for e = 1:GDparams.n_epochs

        %Random shuffle
        rng(400); shuffle = randperm(N);
        trainX = X(:, shuffle); trainY = Y(:, shuffle);

        %Batches
        ord = 1:N/GDparams.n_batch; %ord = randperm(N/GDparams.n_batch);
        for j=1:max(ord) 
            j_start = (ord(j)-1)*GDparams.n_batch + 1;
            j_end = ord(j)*GDparams.n_batch;
            inds = j_start:j_end;
            Xbatch = trainX(:, inds);
            Ybatch = trainY(:, inds);
            [W, b] = MiniBatchGD(Xbatch, Ybatch, GDparams, W, b, lambda);
        end

        %Evaluate
        J_train(e) = ComputeCost(X, Y, W, b, lambda);
        J_val(e) = ComputeCost(Xv, Yv, W, b, lambda);
    end

    %Plot of cost on training & validation set
    figure(1); plot(J_train); hold on; plot(J_val); hold off; 
    xlim([0 e]); ylim([0 ]); %ylim([min(min(J_train), min(J_val)) max(max(J_train), max(max(J_val)))]);
    title('Total loss'); xlabel('Epoch'); ylabel('Loss'); grid on;
    legend({'Training loss','Validation loss'},'Location','northeast');

    %Accuracy on test set
    A = ComputeAccuracy(Xt, yt, W, b)*100; 
    sprintf('Accuracy on test data is %2.2f %',A)

    %Class templates
    s_im{10} = zeros(32,32,3);
    for i=1:10
        im = reshape(W(i, :), 32, 32, 3);
        s_im{i}= (im - min(im(:))) / (max(im(:)) - min(im(:)));
        s_im{i} = permute(s_im{i}, [2, 1, 3]);
    end
    figure(2); title('Class Template images'); montage(s_im, 'Size', [1,10]);
end

function [X, Y, y] = LoadBatch(filename)

    A = load(filename); 
    X = double(A.data')./255;% dxN 3072x10,000
    Y = bsxfun(@eq, 1:10, A.labels+1)';% KxN 10x10,000
    y = (A.labels + 1)';% 1xN 1x10,000

end

function [W, b] = InitParam(k, d)

    W = zeros(k,d);
    rng(400);
    for i=1:k
        W(i,:) = 0.01.*randn(1,d);
    end
    b = 0.01.*randn(k,1); %W = 0.01.*randn(10,3072);
    % W: 10x3072, b: 10x1

end

function P = EvaluateClassifier(X, W, b)

    %SOFTMAX(s) = exp(s)/1T exp(s);
    s = W*X + b; % Kxd*dxN + Kx1 = KxN
    P = softmax(s); % KxN

end

function [grad_W, grad_b] = ComputeGradients(X, Y, P, W, lambda)
    % Y or P: KxN, X: dxN, W: Kxd, b: Kx1
    
    %Initialize
    LossW = 0; Lossb = 0;
    
    %Update loop
    N = size(X, 2);
    for i = 1:N % ith image
        g = -(Y(:,i)-P(:,i))'; %NxK
        LossW = LossW + g' * X(:,i)';
        Lossb = Lossb + g';
    end
    
    R = 2*W;
    Jw = (1./size(X, 2)) * (LossW) + lambda.*R;
    Jb = (1./size(X, 2)) * (Lossb);
    
    grad_W = Jw; %Kxd.
    grad_b = Jb; %Kx1
    
end

function [Wstar, bstar] = MiniBatchGD(X, Y, GDparams, W, b, lambda)
    
    %Predict
    P = EvaluateClassifier(X, W, b);
 
    %Compute gradient
    [grad_W, grad_b] = ComputeGradients(X, Y, P, W, lambda);
    
    %Update the parameters W, b
    Wstar = W - GDparams.eta * grad_W;
    bstar = b - GDparams.eta * grad_b;
    
end

function J = ComputeCost(X, Y, W, b, lambda)
    % Y: KxN, X: dxN, W: W: Kxd, b: Kx1, lambda
    
    P = EvaluateClassifier(X, W, b); %KxN
    L = -log(Y' * P); %NxN
    totalLoss = trace(L); %sum(diag(L))
    R = sumsqr(W); %sum(sum(W.*W));
    J = (totalLoss)./ size(X, 2) + lambda.*R;

end

function A = ComputeAccuracy(X, y, W, b)
    % y: 1xN, X: dxN, W: Kxd, b: Kx1, lambda    
    P = EvaluateClassifier(X, W, b); %KxN
    [~, argmax] = max(P);
    c = (argmax == y);
    A = sum(c)/size(c,2);
end

function [rerr] = rerr(ga, gn)
    %Compute relative error
    rerr = sum(sum(abs(ga - gn)./max(eps, abs(ga) + abs(gn))))./ numel(ga);
end
